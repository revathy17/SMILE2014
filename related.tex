\section{Related Work}
\label{sec:RelatedWork}
There have been two main approaches in addressing the problem of location identification of a twitter user: \begin{inparaenum}[(1)] 
\item Using the content of the tweets: based on the premise that the online content of a user is influenced by the geographical location of the user
\item Using the network information of the user: based on the assumption that the location of the people in a user's network and their online interaction can be used to determine the user's location.
\end{inparaenum}

Content-based location detection relies on a significantly large training dataset to build a statistical model that identifies words with a local scope. Use of these words in tweets are then used to narrow down the location of any user. Cheng et al. \cite{cheng2010you} proposed a probabilistic framework for estimating a Twitter user's city-level location based on the content of approximately 1000+ tweets of each user. The locality of terms was determined by its spatial variation across united states. Their approach on a test dataset of 130689 users with 1000+ tweets each, could locate 51\% of the users within 100 miles. The disadvantage of this approach was the assumption that a "term" is spatially significant to only one location/city. This challenge was addressed by Chang et al.~\cite{chang2012phillies} by modeling the variations as a Gausian mixture model. Their tests on the same test dataset showed an accuracy (within 100 miles) of 0.499 with 509.3 miles of average error distance.
%\cite{ferrara2012web} created language models at different granularity levels from zip code to country level using a training dataset of 5.8 million geotagged tweets.
 %They reported their results on two datasets - SPRITZER containing 5\% of the public twitter stream of 4 weeks and FIREHOSE containing 700,000 tweets from the Twitter Firehose. At the city-level, they reported an accuracy of 65.7\% and 29.8\% on the SPRITZER and the FIREHOSE dataset respectively.

Network based solutions requires the network information of a given user. McGee et al. \cite{mcgee2013location} train an SVM classifier with features based on the information of users' followers-followees who have their location information available. They tested their approach on a random sample of 1000 users and reported 50.08\%accuracy at the city level. However, the limitation of a network-based approaches is the availability of location information of people in the appropriate user's network.   
%Their training dataset consisted of 1.6 million twitter users and their network information. On a test dataset of 249,584 users, they reported 63.9\% accuracy in determining the location within 25 miles. Rout et al. \cite{rout2013s} formulated this task as a classification task and trained an SVM classifier on twitter users with known location, to use a person's social network to locate them. Their training dataset contained 200,000 twitter users. They tested their approach on a random sample of 1000 users and reported 50.08\%accuracy at the city level.

The above mentioned approaches require prior training dataset (of either the content or network), which can be a bottleneck during disaster management. Our goal is to overcome this requirement of training data for each city by leveraging Wikipedia as the knowledge source.
%citeInferring the location of twitter messages based on user relationships.