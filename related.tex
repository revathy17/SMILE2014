\section{Related Work}
\label{sec:RelatedWork}
%There have been two main approaches in addressing the problem of location identification of a twitter user: \begin{inparaenum}[(1)] 
%\item Using the content of the tweets,
%\item Using the network information of the user. 
%\end{inparaenum}

Predicting location of Twitter users has take two directions \begin{inparaenum}[(1)] \item Network based: based on the asumption that users are connected to most people from his/her location. \item Content-based: based on the premise that the online content of a user is influenced by the geographical location of the user \end{inparaenum}

Content-based location detection relies on a significantly large training dataset to build a statistical model that identifies words with a local scope. Use of these words in tweets are then used to narrow down the location of any user.  
Cheng et al. \cite{cheng2010you} proposed a probabilistic framework for estimating a Twitter user's city-level location based on the content of approximately 1000+ tweets of each user. The locality of terms in probabilistic model was determined by its spatial variation (cite spatial variation) across united states. Their approach on a dataset of X users with X+ tweets performed with an accuracy of X within Y miles. One disadvantage of this approach was the assumption that a "term" is spatially significant to a particular location/city. This challenge was addressed by Phillies work [cite] by modeling the variations as Gausian mixture model, which in turn provided with better results. The results on the same dataset used by Chen et al showed an improvement of X within Y miles.      
\cite{ferrara2012web} created language models at different granularity levels from zip code to country level using a training dataset of 5.8 million geotagged tweets.
 %They reported their results on two datasets - SPRITZER containing 5\% of the public twitter stream of 4 weeks and FIREHOSE containing 700,000 tweets from the Twitter Firehose. At the city-level, they reported an accuracy of 65.7\% and 29.8\% on the SPRITZER and the FIREHOSE dataset respectively.

Network based solutions requires the network information of a given user. McGee et al. \cite{mcgee2013location} train an SVM classifier with features based on the information of users' followers-followees who have their location information available. They tested their approach on a random sample of 1000 users and reported 50.08\%accuracy at the city level. However, the limitation of these network-based approaches is the availability of location information of people in the appropriate user's network.   
%Their training dataset consisted of 1.6 million twitter users and their network information. On a test dataset of 249,584 users, they reported 63.9\% accuracy in determining the location within 25 miles. Rout et al. \cite{rout2013s} formulated this task as a classification task and trained an SVM classifier on twitter users with known location, to use a person's social network to locate them. Their training dataset contained 200,000 twitter users. They tested their approach on a random sample of 1000 users and reported 50.08\%accuracy at the city level.

The above mentioned approaches require prior training dataset (either content or netowrk), which forms a bottle neck during disaster management. The approach presented in the paper intends to overcome this requirement of training data for each city by leveraging Wikipedia as the knowledge source.
%citeInferring the location of twitter messages based on user relationships.